Adam optimizor : 1) Momentum; 2) RSMProp (Root Mean Square Propagation) https://www.geeksforgeeks.org/intuition-of-adam-optimizer/
  α: learning rate (0.001)
  β1 & β2: decay rates of average of gradients (β1 = 0.9 & β2 = 0.999)
  ϵ: a small constant to avoid 'division by 0' (10-8))

Adam VS. AdamW: Adam + Weight decay (L2 normalization)

